\#RL

Deterministic Policy Gradient Algorithms

Link - http://proceedings.mlr.press/v32/silver14.pdf

````bibtex
@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}
````

[Bibliography](../Bibliography.md)

---

Stochastic policy gradients integrates over state and action spaces.
Deterministc policy gradients integrate over only the state space.

Basic Idea $\rightarrow$ Choose actions according to stochastic behaviour policy (for exploration), but learn a deterministic target policy (exploit efficiency of DPG).

Policy gradient algorithms aim to adjust the parameters $\theta$ of the policy in the direction of the performance gradient $\nabla\_\\theta J(\pi\_\\theta)$.

The state distribution $p^\pi (s)$ depends on the policy parameters, but the policy gradient does not depend on the gradient of the state distribution.

Actor - Adjusts parameters $\theta$ of the stochastic policy $\pi\_\\theta (s)$ by stochastic gradient ascent. Action-value function $Q^w(s, a)$ is used with parameter vector $w$.
Critic - Estimates the action-value function $Q^w(s, a) \approx Q^\pi(s, a)$ using a policy evaluation algorithm (eg: temporal-difference learning). 

Check [policy-gradients-and-actor-critics](../Knowledge/policy-gradients-and-actor-critics.md) for more information on policy gradients and actor-critic methods.

## Theorem 1

Implies that $\nabla\_\\theta \mu\_\\theta (s)$ and $\nabla_a Q^\mu(s, a)$ exist and that the deterministic policy gradient exists then,

$\nabla\_\\theta J(\mu\_\\theta) = \mathbb E\_{s \sim p^\mu} \[\\nabla\_\\theta \mu\_\\theta(s) \nabla_a Q^\mu(s, a)|*{a=\mu*\\theta (s)}\]$ 

Parameterise stochastic policy $\pi\_{\mu \theta, \sigma}$ by a deterministic policy $\mu\_\\theta : S \rightarrow A$ with a variance parameter $\sigma = 0$ such that the stochastic policy is equivalent to the deterministic policy, $\pi\_{\mu \theta, 0} \equiv \mu\_\\theta$.

## Theorem 2

$lim\_{\sigma \rightarrow 0} \nabla\_\\theta J(\pi\_{\mu \theta, \sigma}) = \nabla\_\\theta J(\mu\_\\theta)$

As the variance tends to 0, the gradient is that of a deterministic policy.

## On-Policy Deterministic Actor-Critic

Actor adjusts parameters $\theta$ of the determinisitc policy $\mu\_\\theta (s)$ by stochastic gradient ascent.
Critic estimates action-value function $Q^w(s, a) \approx Q^\mu(s, a)$ using SARSA updates to estimate the action-value function.

## Off-Policy Deterministic Actor-Critic

Learns deterministic target policy $\mu\_\\theta(s)$ from trajectories generated by an arbitrary stochastic behaviour policy $\pi(s, a)$.

## Theorem 3

States the condition of when a function approximator $Q^w(s, a)$ would be compatible with a deterministic policy $\mu\_\\theta (s)$.
The section has the proof with it.

For any deterministic policy $\mu\_\\theta(s)$ there always exists a compatible function approximator $Q^w(s, a) = (\alpha - \mu\_\\theta(s))^T \nabla\_\\theta \mu\_\\theta (s)^T w + V^\upsilon(s)$
where $V^\upsilon(s)$ is any differentiable baseline function independent of the action $a$.
