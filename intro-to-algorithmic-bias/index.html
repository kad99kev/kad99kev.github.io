<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.e53586a225ffdde20c86.css" id="gatsby-global-css">code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#ccc;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;text-align:left;white-space:pre;word-break:normal;word-spacing:normal}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#2d2d2d}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.block-comment,.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#999}.token.punctuation{color:#ccc}.token.attr-name,.token.deleted,.token.namespace,.token.tag{color:#e2777a}.token.function-name{color:#6196cc}.token.boolean,.token.function,.token.number{color:#f08d49}.token.class-name,.token.constant,.token.property,.token.symbol{color:#f8c555}.token.atrule,.token.builtin,.token.important,.token.keyword,.token.selector{color:#cc99cd}.token.attr-value,.token.char,.token.regex,.token.string,.token.variable{color:#7ec699}.token.entity,.token.operator,.token.url{color:#67cdcc}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.token.inserted{color:green}</style><meta name="generator" content="Gatsby 3.6.2"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><style data-styled="" data-styled-version="5.3.0">.eEfnqX{top:0;left:0;margin:0 auto;display:block;width:100%;z-index:1000;background-color:var(--color-primaryAlpha);font-weight:700;}/*!sc*/
@media (min-width:700px){.eEfnqX{position:fixed;}}/*!sc*/
data-styled.g2[id="Header__HeaderWrapper-sc-1glvhxp-0"]{content:"eEfnqX,"}/*!sc*/
.DXWyk{font-weight:700;margin-left:auto;margin-right:auto;height:60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;max-width:1200px;z-index:1000;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;overflow-x:auto;overflow-y:hidden;white-space:nowrap;padding:0px 10px;}/*!sc*/
data-styled.g3[id="Header__HeaderNav-sc-1glvhxp-1"]{content:"DXWyk,"}/*!sc*/
.dauWMS{display:none;-webkit-box-align:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media (min-width:700px){.dauWMS{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
data-styled.g4[id="Header__HeaderLinksContainer-sc-1glvhxp-2"]{content:"dauWMS,"}/*!sc*/
.bhcqGr{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--color-white);border:0;margin:0;padding:8px 10px;min-width:42px;z-index:10;}/*!sc*/
.Header__HeaderLink-sc-1glvhxp-3 + .Header__HeaderLink-sc-1glvhxp-3{margin-left:0.7rem;}/*!sc*/
data-styled.g5[id="Header__HeaderLink-sc-1glvhxp-3"]{content:"bhcqGr,"}/*!sc*/
.gXXeCg{padding-left:0;}/*!sc*/
data-styled.g6[id="Header__HeaderLinkTitle-sc-1glvhxp-4"]{content:"gXXeCg,"}/*!sc*/
.eVctRs{display:block;padding-left:0;}/*!sc*/
data-styled.g7[id="Header__HeaderLinkTitleContent-sc-1glvhxp-5"]{content:"eVctRs,"}/*!sc*/
.jtaTYo{position:absolute;left:-999px;width:1px;height:1px;top:auto;color:var(--color-white);background-color:var(--color-grey700);}/*!sc*/
.jtaTYo:focus{display:inline-block;height:auto;width:auto;position:static;padding:20px 10px;}/*!sc*/
data-styled.g10[id="Header__SkipMainContent-sc-1glvhxp-8"]{content:"jtaTYo,"}/*!sc*/
.laTCrR{z-index:30;top:0px;position:relative;color:var(--color-white);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:transparent;border:none;cursor:pointer;padding:8px 12px;outline:none;-webkit-tap-highlight-color:transparent;}/*!sc*/
@media (min-width:700px){.laTCrR{display:none;}}/*!sc*/
data-styled.g12[id="Header__BurgerButton-sc-1glvhxp-10"]{content:"laTCrR,"}/*!sc*/
.bCxojD{width:24px;top:30px;height:2px;background:var(--color-white);position:absolute;left:0;background:var(--color-white);-webkit-transition:all 250ms cubic-bezier(0.86,0,0.07,1);transition:all 250ms cubic-bezier(0.86,0,0.07,1);}/*!sc*/
.bCxojD::before{content:'';top:-8px;width:24px;height:2px;background:var(--color-white);position:absolute;left:0;-webkit-transform:rotate(0);-ms-transform:rotate(0);transform:rotate(0);-webkit-transition:all 250ms cubic-bezier(0.86,0,0.07,1);transition:all 250ms cubic-bezier(0.86,0,0.07,1);}/*!sc*/
.bCxojD::after{top:8px;content:'';width:24px;height:2px;background:white;position:absolute;left:0;-webkit-transform:rotate(0);-ms-transform:rotate(0);transform:rotate(0);-webkit-transition:all 250ms cubic-bezier(0.86,0,0.07,1);transition:all 250ms cubic-bezier(0.86,0,0.07,1);}/*!sc*/
data-styled.g13[id="Header__BurgerContent-sc-1glvhxp-11"]{content:"bCxojD,"}/*!sc*/
.iTZmMq{text-align:left;padding-top:30px;padding-bottom:50px;background-color:var(--color-primary);color:var(--color-white);padding-left:20px;padding-right:20px;margin:0 auto;}/*!sc*/
.iTZmMq nav{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-flow:row wrap;-ms-flex-flow:row wrap;flex-flow:row wrap;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;max-width:900px;margin:0 auto;}/*!sc*/
.iTZmMq nav .footer-col{-webkit-flex:1 auto;-ms-flex:1 auto;flex:1 auto;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-right:1em;}/*!sc*/
.iTZmMq a{color:var(--color-white);font-weight:bold;}/*!sc*/
.iTZmMq a:hover{color:var(--color-grey200);}/*!sc*/
.iTZmMq .footer-col > p{margin:0;}/*!sc*/
.iTZmMq .footer-title{font-size:0.83em;margin:0 0 1rem;}/*!sc*/
.iTZmMq .footer-item{color:var(--color-white);}/*!sc*/
.iTZmMq .footer-item a{padding:0.25rem 0;display:block;}/*!sc*/
.iTZmMq .footer-heart{color:var(--color-red);}/*!sc*/
.iTZmMq .footer-item-text{padding:0.1rem 0;color:var(--color-white);}/*!sc*/
.iTZmMq .footer-header{-webkit-order:1;-ms-flex-order:1;order:1;margin:0 0.25rem;margin-right:0.25rem;padding:0.25rem;}/*!sc*/
.iTZmMq .footer-column-items{grid-template-columns:1fr;display:grid;}/*!sc*/
@media (max-width:564px){.iTZmMq .footer-col:first-child{width:100%;}}/*!sc*/
data-styled.g14[id="Footer__FooterWrapper-t92llg-0"]{content:"iTZmMq,"}/*!sc*/
*{box-sizing:border-box;margin:0;padding:0;}/*!sc*/
body{font-family:"Montserrat",sans-serif;color:var(--color-text);background-color:var(--color-siteBackground);}/*!sc*/
img{max-width:100%;height:auto;vertical-align:middle;border:0;}/*!sc*/
a{-webkit-text-decoration:none;text-decoration:none;color:var(--color-text);}/*!sc*/
hr{border:0;border-top:1px solid black;margin:50px 0 5px 0;}/*!sc*/
ul,ol{padding-left:2em;margin:1em 0 0 0;}/*!sc*/
*::selection{background-color:var(--color-secondary);}/*!sc*/
data-styled.g15[id="sc-global-hmWUiG1"]{content:"sc-global-hmWUiG1,"}/*!sc*/
.eiSJyb{line-height:1.6;margin:1em 0 0 0;}/*!sc*/
data-styled.g17[id="Commons__Text-sc-1aaxjtz-1"]{content:"eiSJyb,"}/*!sc*/
.eobgui{display:inline-block;color:var(--color-textSecondary);margin:0 4px;}/*!sc*/
.eobgui::before{content:'•';}/*!sc*/
data-styled.g18[id="Commons__Bull-sc-1aaxjtz-2"]{content:"eobgui,"}/*!sc*/
.cbpkEw{text-transform:uppercase;color:var(--color-tertiary);}/*!sc*/
data-styled.g19[id="Commons__ReadingTimeContainer-sc-1aaxjtz-3"]{content:"cbpkEw,"}/*!sc*/
.iIiQHQ{margin:0 0;}/*!sc*/
@media (min-width:700px){.iIiQHQ{margin:60px 0;}}/*!sc*/
data-styled.g20[id="layout__SiteContent-nrr2ov-0"]{content:"iIiQHQ,"}/*!sc*/
.hncJWA{position:relative;border-radius:5px;width:80%;max-width:870px;word-wrap:break-word;background-color:var(--color-wrapperBackground);margin:0px auto 30px auto;top:30px;padding:50px;box-shadow:0 0 0 0,0 6px 12px var(--color-wrapperShadow);}/*!sc*/
@media (max-width:780px){.hncJWA{width:100%;padding:25px;}}/*!sc*/
data-styled.g21[id="Wrapper-xmiwfw-0"]{content:"hncJWA,"}/*!sc*/
.jaVTPk{position:relative;display:table;width:100%;height:500px;overflow:hidden;background-repeat:no-repeat;background-position:center;background-size:cover;}/*!sc*/
data-styled.g26[id="Hero__HeroContainer-sc-1wsaguq-0"]{content:"jaVTPk,"}/*!sc*/
.huJLAN{display:table-cell;vertical-align:middle;text-align:center;width:100%;}/*!sc*/
data-styled.g27[id="Hero__TitleContainer-sc-1wsaguq-1"]{content:"huJLAN,"}/*!sc*/
.kgYWl{font-family:"Rubik",serif;font-weight:500;font-size:3rem;margin:10px 50px;color:var(--color-white);text-shadow:1px 1px 4px rgba(34,34,34,0.85);}/*!sc*/
data-styled.g28[id="Hero__HeroTitle-sc-1wsaguq-2"]{content:"kgYWl,"}/*!sc*/
.eSwQYU{display:inline;color:var(--color-secondary);}/*!sc*/
data-styled.g30[id="TagList__ListContainer-tgjq80-0"]{content:"eSwQYU,"}/*!sc*/
.jCAvVB{text-transform:uppercase;color:var(--color-secondary);}/*!sc*/
.jCAvVB:not(:first-child){margin-left:0.3rem;}/*!sc*/
.jCAvVB:hover{border-bottom:1px dotted var(--color-secondary);}/*!sc*/
.jCAvVB:before{content:'#';}/*!sc*/
data-styled.g31[id="TagList__TagListItemLink-tgjq80-1"]{content:"jCAvVB,"}/*!sc*/
.fAsDiK{text-transform:uppercase;color:var(--color-secondary);}/*!sc*/
.fAsDiK:not(:first-child){margin-left:0.3rem;}/*!sc*/
.fAsDiK:before{content:'#';}/*!sc*/
data-styled.g32[id="TagList__TagListItem-tgjq80-2"]{content:"fAsDiK,"}/*!sc*/
.fgnlCs .author-image{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;display:block;position:absolute;top:-40px;left:50%;margin-left:-40px;width:80px;height:80px;border-radius:100%;overflow:hidden;padding:6px;z-index:2;box-shadow:#ececec 0 0 0 1px;background-color:var(--color-wrapperBackground);}/*!sc*/
.fgnlCs .author-image .img{position:relative;display:block;width:100%;height:100%;background-size:cover;background-position:center center;border-radius:100%;}/*!sc*/
.fgnlCs .author-profile .author-image{position:relative;left:auto;top:auto;width:120px;height:120px;padding:3px;margin:-100px auto 0 auto;box-shadow:none;}/*!sc*/
data-styled.g46[id="Bio__BioWrapper-d7fgaq-0"]{content:"fgnlCs,"}/*!sc*/
.liguIv a{box-shadow:0 2px 0 0 var(--color-secondary);}/*!sc*/
.liguIv a:hover{-webkit-filter:brightness(150%);filter:brightness(150%);box-shadow:none;}/*!sc*/
data-styled.g47[id="Bio__BioText-d7fgaq-1"]{content:"liguIv,"}/*!sc*/
.dHeQnb{color:var(--color-tertiary);}/*!sc*/
data-styled.g51[id="Time__TimeContainer-sc-1u6vd0l-0"]{content:"dHeQnb,"}/*!sc*/
.eXLPHg{margin-bottom:2rem;font-size:0.9em;}/*!sc*/
data-styled.g52[id="ContentHeader__Header-sc-1qtxhip-0"]{content:"eXLPHg,"}/*!sc*/
.iJhWBi{font-family:"Rubik",serif;font-weight:500;font-size:3rem;margin-bottom:0.5rem;color:var(--color-textSecondary);}/*!sc*/
data-styled.g53[id="ContentHeader__HeroTitle-sc-1qtxhip-1"]{content:"iJhWBi,"}/*!sc*/
.bGghBC{line-height:1.6;}/*!sc*/
.bGghBC > h2{color:var(--color-h2);padding-top:3rem;margin-top:3rem;border-top:1px solid black;}/*!sc*/
.bGghBC > h3{padding-top:3rem;}/*!sc*/
.bGghBC > h4{color:var(--color-textSecondary);}/*!sc*/
.bGghBC > h5{color:var(--color-h5);}/*!sc*/
.bGghBC > p{margin:1em 0 0 0;}/*!sc*/
.bGghBC a{fill:var(--color-text);box-shadow:0 2px 0 0 var(--color-secondary);}/*!sc*/
.bGghBC a:hover{-webkit-filter:brightness(150%);filter:brightness(150%);box-shadow:none;}/*!sc*/
.bGghBC a.anchor,.bGghBC a.gatsby-resp-image-link{box-shadow:none;}/*!sc*/
.bGghBC h1 .anchor svg,.bGghBC h2 .anchor svg,.bGghBC h3 .anchor svg,.bGghBC h4 .anchor svg,.bGghBC h5 .anchor svg,.bGghBC h6 .anchor svg{visibility:hidden;margin-left:-16px;}/*!sc*/
.bGghBC h1:hover .anchor svg,.bGghBC h2:hover .anchor svg,.bGghBC h3:hover .anchor svg,.bGghBC h4:hover .anchor svg,.bGghBC h5:hover .anchor svg,.bGghBC h6:hover .anchor svg,.bGghBC h1 .anchor:focus svg,.bGghBC h2 .anchor:focus svg,.bGghBC h3 .anchor:focus svg,.bGghBC h4 .anchor:focus svg,.bGghBC h5 .anchor:focus svg,.bGghBC h6 .anchor:focus svg{visibility:visible;}/*!sc*/
.bGghBC > blockquote{box-sizing:border-box;background-color:var(--color-secondaryContentBackground);border-left:5px solid var(--color-secondary);margin:30px 0px;padding:5px 20px;border-radius:0 8px 8px 0;}/*!sc*/
.bGghBC > blockquote p{margin:0.8em 0;font-style:italic;}/*!sc*/
.bGghBC .gatsby-highlight{border-radius:5px;font-size:15px;line-height:1.7;border-radius:10px;overflow:auto;tab-size:1.5em;margin:1.5em -1.5em;}/*!sc*/
@media (max-width:500px){.bGghBC .gatsby-highlight{border-radius:0;margin-left:-25px;margin-right:-25px;}}/*!sc*/
.bGghBC .gatsby-highlight > pre{border:0;margin:0;padding:1;}/*!sc*/
.bGghBC .gatsby-highlight pre[class*='language-']{float:left;min-width:100%;}/*!sc*/
.bGghBC .gatsby-highlight-code-line{background-color:var(--color-darkBlue);display:block;margin-right:-1em;margin-left:-1em;padding-right:1em;padding-left:0.75em;border-left:0.25em solid var(--color-yellow);}/*!sc*/
.bGghBC h1 > code.language-text,.bGghBC h2 > code.language-text,.bGghBC h3 > code.language-text,.bGghBC h4 > code.language-text,.bGghBC h5 > code.language-text,.bGghBC h6 > code.language-text,.bGghBC a > code.language-text,.bGghBC p > code.language-text,.bGghBC li > code.language-text,.bGghBC em > code.language-text,.bGghBC strong > code.language-text{background:var(--color-beige);color:#222222cc;padding:0 3px;font-size:0.94em;border-radius:0.2rem;word-wrap:break-word;}/*!sc*/
.bGghBC code{word-wrap:break-word;}/*!sc*/
.bGghBC table{margin-top:1em;margin-bottom:1em;border-collapse:collapse;overflow:hidden;}/*!sc*/
.bGghBC table th,.bGghBC table td{padding:0.5em;background-color:var(--color-secondaryContentBackground);}/*!sc*/
.bGghBC table tr{border-bottom:2px solid var(--color-white);}/*!sc*/
.bGghBC table tbody tr:last-child{border-bottom:none;}/*!sc*/
data-styled.g54[id="Content__ContentBody-p0bolz-0"]{content:"bGghBC,"}/*!sc*/
.fKQyLx{padding:0 30px 30px;}/*!sc*/
@media only screen and (max-width:500px){.fKQyLx{padding:0;}}/*!sc*/
data-styled.g55[id="Article__ArticleWrapper-jwdbi9-0"]{content:"fKQyLx,"}/*!sc*/
.lmhsJW{position:relative;margin:6rem 0 0;padding:3rem 0 0;border-top:1px solid black;}/*!sc*/
data-styled.g56[id="Article__ArticleFooter-jwdbi9-1"]{content:"lmhsJW,"}/*!sc*/
.dMxoIk{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;max-width:870px;width:80%;margin:0px auto 30px auto;top:20px;position:relative;}/*!sc*/
@media (max-width:780px){.dMxoIk{width:100%;padding:25px;}}/*!sc*/
data-styled.g57[id="PrevNextPost__PreviewContainer-sc-1r1qyep-0"]{content:"dMxoIk,"}/*!sc*/
.dOZAZy{cursor:pointer;-webkit-flex:1 1 300px;-ms-flex:1 1 300px;flex:1 1 300px;background-color:var(--color-secondaryContentBackground);box-shadow:0 0 0 0,0 6px 12px rgba(0,0,0,0.1);margin:20px 20px;border-radius:5px;}/*!sc*/
.dOZAZy:hover{box-shadow:0 0 0 0,0 6px 12px var(--color-grey300);-webkit-transition:all 0.3s ease;transition:all 0.3s ease;-webkit-transform:translate3D(0,-1px,0);-ms-transform:translate3D(0,-1px,0);transform:translate3D(0,-1px,0);}/*!sc*/
@media (min-width:780px){.dOZAZy:first-child{margin-left:0;}.dOZAZy:last-child{margin-right:0;}}/*!sc*/
data-styled.g58[id="PrevNextPost__Preview-sc-1r1qyep-1"]{content:"dOZAZy,"}/*!sc*/
.kSsoVo{width:auto;height:200px;background:#c5d2d9 no-repeat 50%;background-size:cover;border-radius:5px 5px 0 0;}/*!sc*/
data-styled.g59[id="PrevNextPost__PreviewCover-sc-1r1qyep-2"]{content:"kSsoVo,"}/*!sc*/
.hPZqEh{padding:20px;}/*!sc*/
.hPZqEh header{padding:0 0 10px 0;}/*!sc*/
.hPZqEh section{padding-bottom:10px;}/*!sc*/
.hPZqEh footer{font-size:0.8em;}/*!sc*/
data-styled.g60[id="PrevNextPost__PreviewContent-sc-1r1qyep-3"]{content:"hPZqEh,"}/*!sc*/
</style><title data-react-helmet="true">An Introduction to Algorithmic Bias | @kad99kev</title><link data-react-helmet="true" href="https://fonts.googleapis.com/css?family=Lato:400,700&amp;display=swap" rel="stylesheet"/><link data-react-helmet="true" href="https://fonts.googleapis.com/css?family=Rubik:400,700&amp;display=swap" rel="stylesheet"/><link data-react-helmet="true" rel="canonical" href="https://kad99kev.github.io/intro-to-algorithmic-bias"/><meta data-react-helmet="true" name="description" content="Humans write algorithms and code that run on data collected from the
real world. Together, they may mimic or exaggerate any preexisting bias…"/><meta data-react-helmet="true" property="og:url" content="https://kad99kev.github.io/intro-to-algorithmic-bias"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="An Introduction to Algorithmic Bias | @kad99kev"/><meta data-react-helmet="true" property="og:description" content="Humans write algorithms and code that run on data collected from the
real world. Together, they may mimic or exaggerate any preexisting bias…"/><meta data-react-helmet="true" property="og:image" content="https://kad99kev.github.io/static/0856c7bec18175bbfaf6bf919c2fbb10/algorithmic.webp"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:creator" content="kad99kev"/><meta data-react-helmet="true" name="twitter:title" content="An Introduction to Algorithmic Bias | @kad99kev"/><meta data-react-helmet="true" name="twitter:description" content="Humans write algorithms and code that run on data collected from the
real world. Together, they may mimic or exaggerate any preexisting bias…"/><meta data-react-helmet="true" name="twitter:image" content="https://kad99kev.github.io/static/0856c7bec18175bbfaf6bf919c2fbb10/algorithmic.webp"/><script data-react-helmet="true" type="application/ld+json">[{"@context":"http://schema.org","@type":"WebSite","url":"https://kad99kev.github.io/intro-to-algorithmic-bias","name":"An Introduction to Algorithmic Bias | @kad99kev","alternateName":"An Introduction to Algorithmic Bias | @kad99kev"},{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://kad99kev.github.io/intro-to-algorithmic-bias","name":"An Introduction to Algorithmic Bias | @kad99kev","image":"https://kad99kev.github.io/static/0856c7bec18175bbfaf6bf919c2fbb10/algorithmic.webp"}}]},{"@context":"http://schema.org","@type":"BlogPosting","url":"https://kad99kev.github.io/intro-to-algorithmic-bias","name":"An Introduction to Algorithmic Bias | @kad99kev","alternateName":"An Introduction to Algorithmic Bias | @kad99kev","headline":"An Introduction to Algorithmic Bias | @kad99kev","image":{"@type":"ImageObject","url":"https://kad99kev.github.io/static/0856c7bec18175bbfaf6bf919c2fbb10/algorithmic.webp"},"description":"Humans write algorithms and code that run on data collected from the\nreal world. Together, they may mimic or exaggerate any preexisting bias…","author":{"@type":"Person"},"publisher":{"@type":"Organization"},"mainEntityOfPage":{"@type":"WebSite","@id":"https://kad99kev.github.io/intro-to-algorithmic-bias"}}]</script><link rel="icon" href="/favicon-32x32.png?v=2db32f80550db4f421de2422943a54f3" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><meta name="theme-color" content="#222222"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=2db32f80550db4f421de2422943a54f3"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=2db32f80550db4f421de2422943a54f3"/><style>html { 
--color-grey100: undefined;
--color-grey200: undefined;
--color-grey300: undefined;
--color-grey400: undefined;
--color-grey500: undefined;
--color-grey600: undefined;
--color-grey700: undefined;
--color-grey800: undefined;
--color-grey900: undefined;
--color-lightYellow: undefined;
--color-blueGreyed: undefined;
--color-darkBlue: undefined;
--color-beige: undefined;
--color-red: undefined;
--color-yellow: undefined;
--color-black: undefined;
--color-white: undefined;
--color-lightBackground: undefined;
--color-lightWrapperBackground: undefined;
--color-lightSecondaryWrapperBackground: undefined;
--color-lightFontColour: undefined;
--color-lightSecondaryFontColour: undefined;
--color-lighth5: undefined;
--color-lightLinkHashtag: undefined;
--color-lightTime: undefined;
--color-darkBackground: undefined;
--color-darkWrapperBackground: undefined;
--color-darkSecondaryWrapperBackground: undefined;
--color-darkFontColour: undefined;
--color-darkSecondaryFontColour: undefined;
--color-darkh5: undefined;
--color-darkLinkHashtag: undefined;
--color-darkTime: undefined;
--color-text: #1e2022;
--color-textSecondary: #5b2929;
--color-siteBackground: #f6f6f6;
--color-wrapperBackground: #ffe2e2;
--color-secondaryContentBackground: #fff1f1;
--color-wrapperShadow: #0000001a;
--color-primary: #3E4047;
--color-primaryAlpha: rgba(32, 35, 42, 0.85);
--color-secondary: #0077b6;
--color-tertiary: #14746f;
--color-h2: #3E4047;
--color-h5: #922020; }</style><link as="script" rel="preload" href="/webpack-runtime-7190c2abaa7047efd8ad.js"/><link as="script" rel="preload" href="/framework-ea89757ffca28d78b958.js"/><link as="script" rel="preload" href="/app-2cc33f53fc0cb54ce6df.js"/><link as="script" rel="preload" href="/3a2f1ab89072400eb78a258db61265f1964eff8a-3380ef0e1c3d3a1e650c.js"/><link as="script" rel="preload" href="/6dae301c9709e8c5899acc3516b7dd8f43382179-66d819d73e5ed43a1a17.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-6753f08586774d75dbac.js"/><link as="fetch" rel="preload" href="/page-data/intro-to-algorithmic-bias/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/1956263691.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/4156811642.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><script>!function(){const r=window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("color-mode");let a="light";a="string"==typeof e?e:r?"dark":"light";let t=document.documentElement;t.style.setProperty("--initial-color-mode",a),Object.entries({grey100:"#ECECED",grey200:"#CFCFD1",grey300:"#B2B3B5",grey400:"#78797E",grey500:"#3E4047",grey600:"#383A40",grey700:"#25262B",grey800:"#1C1D20",grey900:"#131315",lightYellow:"#f9e892",blueGreyed:"#546c77",darkBlue:"#022a4b",beige:"#fff9d9",red:"#ff0000",yellow:"#ffdc4e",black:"#161a1d",white:"#f5f3f4",lightBackground:"#f6f6f6",lightWrapperBackground:"#ffe2e2",lightSecondaryWrapperBackground:"#fff1f1",lightFontColour:"#1e2022",lightSecondaryFontColour:"#5b2929",lighth5:"#922020",lightLinkHashtag:"#0077b6",lightTime:"#14746f",darkBackground:"#1e2022",darkWrapperBackground:"#52616b",darkSecondaryWrapperBackground:"#7c7f88",darkFontColour:"#f5f3f4",darkSecondaryFontColour:"#d3d3d3",darkh5:"#aaaaaa",darkLinkHashtag:"#48cae4",darkTime:"#88d4ab",text:{light:"#1e2022",dark:"#f5f3f4"},textSecondary:{light:"#5b2929",dark:"#d3d3d3"},siteBackground:{light:"#f6f6f6",dark:"#1e2022"},wrapperBackground:{light:"#ffe2e2",dark:"#52616b"},secondaryContentBackground:{light:"#fff1f1",dark:"#7c7f88"},wrapperShadow:{light:"#0000001a",dark:"#00000000"},primary:{light:"#3E4047",dark:"#3E4047"},primaryAlpha:{light:"rgba(32, 35, 42, 0.85)",dark:"rgba(32, 35, 42, 0.85)"},secondary:{light:"#0077b6",dark:"#48cae4"},tertiary:{light:"#14746f",dark:"#88d4ab"},h2:{light:"#3E4047",dark:"#ffdc4e"},h5:{light:"#922020",dark:"#aaaaaa"}}).forEach(([r,e])=>{const d="--color-"+r;"object"==typeof e?t.style.setProperty(d,e[a]):t.style.setProperty(d,e)})}();</script><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><header class="Header__HeaderWrapper-sc-1glvhxp-0 eEfnqX"><nav class="Header__HeaderNav-sc-1glvhxp-1 DXWyk"><a href="#main-content" class="Header__SkipMainContent-sc-1glvhxp-8 jtaTYo">Skip to main content</a><a aria-label="View home page" class="Header__HeaderLink-sc-1glvhxp-3 Header__HeaderLinkTitle-sc-1glvhxp-4 bhcqGr gXXeCg" href="/"><span class="Header__HeaderLinkTitleContent-sc-1glvhxp-5 eVctRs">@kad99kev</span></a><div class="Header__HeaderLinksContainer-sc-1glvhxp-2 dauWMS"><a aria-label="View Blog page" class="Header__HeaderLink-sc-1glvhxp-3 bhcqGr" href="/">Blog</a><a aria-label="View Education page" class="Header__HeaderLink-sc-1glvhxp-3 bhcqGr" href="/education">Education</a><a aria-label="View Experience page" class="Header__HeaderLink-sc-1glvhxp-3 bhcqGr" href="/experience">Experience</a><a aria-label="View Awards page" class="Header__HeaderLink-sc-1glvhxp-3 bhcqGr" href="/awards">Awards</a><a aria-label="View About page" class="Header__HeaderLink-sc-1glvhxp-3 bhcqGr" href="/about-me">About</a></div><button aria-label="open menu" class="Header__BurgerButton-sc-1glvhxp-10 laTCrR"><div class="Header__BurgerContent-sc-1glvhxp-11 bCxojD"></div></button></nav></header><div class="layout__SiteContent-nrr2ov-0 iIiQHQ"><div style="background-image:url(&quot;/static/0856c7bec18175bbfaf6bf919c2fbb10/algorithmic.webp&quot;)" class="Hero__HeroContainer-sc-1wsaguq-0 jaVTPk"><div class="Hero__TitleContainer-sc-1wsaguq-1 huJLAN"><h1 class="Hero__HeroTitle-sc-1wsaguq-2 kgYWl"></h1></div></div><main role="main" id="main-content" class="Wrapper-xmiwfw-0 hncJWA"><article class="Article__ArticleWrapper-jwdbi9-0 fKQyLx"><section><header class="ContentHeader__Header-sc-1qtxhip-0 eXLPHg"><h1 class="ContentHeader__HeroTitle-sc-1qtxhip-1 iJhWBi">An Introduction to Algorithmic Bias</h1><time class="Time__TimeContainer-sc-1u6vd0l-0 dHeQnb">January 6, 2022</time><span class="Commons__Bull-sc-1aaxjtz-2 eobgui"></span><div class="TagList__ListContainer-tgjq80-0 eSwQYU"><a class="TagList__TagListItemLink-tgjq80-1 jCAvVB" href="/tags/ai">ai</a>, <a class="TagList__TagListItemLink-tgjq80-1 jCAvVB" href="/tags/ethics">ethics</a></div></header><div class="Content__ContentBody-p0bolz-0 bGghBC"><p>Humans write algorithms and code that run on data collected from the
real world. Together, they may mimic or exaggerate any preexisting bias.
This is what we call Algorithmic Bias. We could try to avoid collecting
data that segregate people based on their gender, race or religion. But,
they may still materialize as correlated features. For example,
purchasing records could correlate to gender and zip codes could
correlate to race. Bias could also arise due to the lack of relevant
data. For example, in the famous dataset - “Labeled Faces in the Wild”
<a href="#1">[1]</a> 83.5% of images are of white people <a href="#2">[2]</a>. It also contains a
limited number of children, no babies, very few adults above the age of
80, and a small proportion of women. It also states that several
ethnicities have very little or no representation at all. The creators
of the LFW dataset mention that the dataset is not meant for commercial
applications. But are companies active in trying to identify any
pre-existing biases in their datasets? Are they aware of how their
algorithms interact with society? Algorithmic systems shape our lives,
influencing our opportunities in employment, education and finance. The
data that we unknowingly provide them with often fuel these systems.
This has been the source of the flame that has destroyed so many
innocent lives.</p><p>In 2016, ProPublica published its analysis of COMPAS (Correctional
Offender Management Profiling for Alternative Sanctions). This was an
algorithm that not only assesses the risk of committing a crime in the
future but also around two dozen so-called “criminogenic needs”. It
ranks the defendants either as low, medium or high risk in each category
<a href="#3">[3]</a>. The algorithm predicted that blacks are twice as likely as whites
to be labelled a higher risk and not re-offend. But it would make the
opposite mistake among whites. They were more likely labelled to be
lower risk but go on and commit more crimes.</p><figure><span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:504px">
      <span class="gatsby-resp-image-background-image" style="padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABQklEQVQoz5VR23KDIBT035qHVNRwUS5GTZNp2o5NjIKCYqbf3qKpTfvWmZ3DsrAHWLwwFmEsAsJn8i9405D6MAGIui6YhYQHmDks5A+fpjdzEAsfJQBTgJmPqI+o8xMOEF22+og6A+Fgds7mgHAsCoDZehOz4sDyAxHFI0wmPY++HxVvdwC51lEiwHJyQDjN9yHhD/6mklpq2w7X57J6qxqph1PTlbUqL6obrq/n+vh+afQA6RbSbUC4u3aUpAGmKwBPddu0fdP1ytimc0RqO9VhrlIPyti6NfuXcg0T7xYd4SuAzlIrM7b9qIyVemh7x6WxylhHtJXaKjOa8eNYVutN7C25B5hhUcTpjqTFDCwKzDPMc5L+0okoIMtu1/7xzyFjNgOxLMme5jjvdXD/VQuiJF0QxgKyDIsCsexeX1a/zJ/uPdP0J9mXCwAAAABJRU5ErkJggg==&#x27;);background-size:cover;display:block"></span>
  <picture>
          <source srcSet="/static/8cb8d97d3a179be9277c8dd45ebedbd9/cbe2e/black-defendant.webp 148w,/static/8cb8d97d3a179be9277c8dd45ebedbd9/3084c/black-defendant.webp 295w,/static/8cb8d97d3a179be9277c8dd45ebedbd9/062aa/black-defendant.webp 504w" sizes="(max-width: 504px) 100vw, 504px" type="image/webp"/>
          <source srcSet="/static/8cb8d97d3a179be9277c8dd45ebedbd9/12f09/black-defendant.png 148w,/static/8cb8d97d3a179be9277c8dd45ebedbd9/e4a3f/black-defendant.png 295w,/static/8cb8d97d3a179be9277c8dd45ebedbd9/08115/black-defendant.png 504w" sizes="(max-width: 504px) 100vw, 504px" type="image/png"/>
          <img class="gatsby-resp-image-image" src="/static/8cb8d97d3a179be9277c8dd45ebedbd9/08115/black-defendant.png" alt="Black Defendants’ Risk Scores" title="Black Defendants’ Risk Scores" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0"/>
        </picture>
    </span><figcaption style="text-align:center" aria-hidden="true">Black Defendants’ Risk Scores</figcaption></figure><figure><span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:504px">
      <span class="gatsby-resp-image-background-image" style="padding-bottom:50%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABOUlEQVQoz42RbW+CMBSF+W+axdGifaFvCAjOOOcUxAJFZNtf36qk8cuWJU/S5t6ce09PvSBUQagglUGoAJH3yz/x7MEiH7OngKxfD9lmN5vTgEpIxF/cdlgxtDv5xF9U7eVYNVOA5iyybWqN/IYVQyqpSiERE4AKbfalni1CSAUgwsfc+vplyijmSR5QOfFRodtSm6rpsExolLI4f0bsPmUUONybrUnCpwAdz23VdNr0pTalbrXp347VarNTqxdIbAo+5j7mduLNlDdGR+UUoFPd1d21uQx1N2jT15fBDF9N/2Gun2VtDqd6X5y370W83vI4B0R4LndIBFVpGK1olN4hKsEixiImKmHLbCTORJzbmKj0Hv8NEOHwMUdiyZYZUYmrOAARD7ZvzFnkCEKF7M4Uy+Sx7ro/4m96Xc5ECu5nugAAAABJRU5ErkJggg==&#x27;);background-size:cover;display:block"></span>
  <picture>
          <source srcSet="/static/b4666fea93ba85a4aadcee23c4a443ca/cbe2e/white-defendant.webp 148w,/static/b4666fea93ba85a4aadcee23c4a443ca/3084c/white-defendant.webp 295w,/static/b4666fea93ba85a4aadcee23c4a443ca/062aa/white-defendant.webp 504w" sizes="(max-width: 504px) 100vw, 504px" type="image/webp"/>
          <source srcSet="/static/b4666fea93ba85a4aadcee23c4a443ca/12f09/white-defendant.png 148w,/static/b4666fea93ba85a4aadcee23c4a443ca/e4a3f/white-defendant.png 295w,/static/b4666fea93ba85a4aadcee23c4a443ca/08115/white-defendant.png 504w" sizes="(max-width: 504px) 100vw, 504px" type="image/png"/>
          <img class="gatsby-resp-image-image" src="/static/b4666fea93ba85a4aadcee23c4a443ca/08115/white-defendant.png" alt="White Defendants’ Risk Scores" title="White Defendants’ Risk Scores" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0"/>
        </picture>
    </span><figcaption style="text-align:center" aria-hidden="true">White Defendants’ Risk Scores</figcaption></figure><p>In 2018, researchers conducted a study with Convolution Neural Networks
(CNN). It detected potentially cancerous skin lesions better than the
study group that included 58 dermatologists. Yet, the data used to train
the CNN came from fair-skinned populations in the US, Australia and
Europe. As a result, people of colour could either be misdiagnosed with
nonexistent skin cancers or they could be completely missed <a href="#4">[4]</a>. In
2019, Facebook was sued for letting advertisers target their ads based
on race, gender and religion. It included postings for preschool
teachers and secretaries which they showed to a higher fraction of
women. And, they showed postings for janitors and taxi drivers to a
higher proportion of minorities. White users were shown more ads on home
sales, while minorities were shown ads for rentals <a href="#5">[5]</a>. The way these
systems are established, create a positive feedback loop. Every
incorrect prediction, every false sentencing and every targeted
advertisement adds another data point to the already biased system.</p><figure><span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:590px">
      <span class="gatsby-resp-image-background-image" style="padding-bottom:50.67567567567568%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEDAgT/xAAWAQEBAQAAAAAAAAAAAAAAAAACAAH/2gAMAwEAAhADEAAAAe7aeqpIh//EABkQAQACAwAAAAAAAAAAAAAAAAECEAARIf/aAAgBAQABBQJ4RmZumv/EABgRAAIDAAAAAAAAAAAAAAAAAAAhAQIR/9oACAEDAQE/AbKEaf/EABgRAAIDAAAAAAAAAAAAAAAAAAARARIx/9oACAECAQE/AU9KQf/EABcQAAMBAAAAAAAAAAAAAAAAAAARMSD/2gAIAQEABj8CKyY//8QAGxAAAwACAwAAAAAAAAAAAAAAAAEREDFBYcH/2gAIAQEAAT8h3JehKKpW0ieWx4Ef/9oADAMBAAIAAwAAABAf3//EABYRAQEBAAAAAAAAAAAAAAAAAAARcf/aAAgBAwEBPxDUW//EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPxBByD//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMXFRYf/aAAgBAQABPxBLBJGgd3ACTksVb8gl5F+RJscTV7P/2Q==&#x27;);background-size:cover;display:block"></span>
  <picture>
          <source srcSet="/static/39fc2a5282dc2d0337a8dfaa7a03aad8/cbe2e/bmj.webp 148w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/3084c/bmj.webp 295w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/5ca24/bmj.webp 590w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/dad35/bmj.webp 885w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/2baf0/bmj.webp 1180w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/b84eb/bmj.webp 2000w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcSet="/static/39fc2a5282dc2d0337a8dfaa7a03aad8/a80bd/bmj.jpg 148w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c91a/bmj.jpg 295w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c72d/bmj.jpg 590w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/a8a14/bmj.jpg 885w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/fbd2c/bmj.jpg 1180w,/static/39fc2a5282dc2d0337a8dfaa7a03aad8/3acf0/bmj.jpg 2000w" sizes="(max-width: 590px) 100vw, 590px" type="image/jpeg"/>
          <img class="gatsby-resp-image-image" src="/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c72d/bmj.jpg" alt="White Defendants’ Risk Scores" title="White Defendants’ Risk Scores" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0"/>
        </picture>
    </span><figcaption style="text-align:center" aria-hidden="true">The cascading effect of AI.
<br/>
Image source: The British Medical Journal <a href="#6">[6]</a></figcaption></figure><p>The first step that we as developers can take is understanding our
responsibility. A study was conducted in 2019 where one on one
interviews were conducted with around 35 ML practitioners. Most of the
interviewees reported that their teams do not have any protocols in
place to support the collection and curation of a balanced or
representative dataset. Often these teams do not discover serious
fairness issues until they receive customer feedback <a href="#7">[7]</a>. A biased
society often leads to a biased AI, and programmers are merely a part of
society. This same AI that we use so often, is developed by human
programmers. Hence, we must recognize our own biases to avoid
incorporating them into the AI systems that we develop. So, how can we
combat algorithmic bias? There has been a lot of discussion around the
power of open source and its communities. We could find great potential
in its technologies and methodologies in the fight against algorithmic
bias. We have AI research labs from companies like Google (Google Brain)
and Facebook (Facebook AI Research) working on open-sourced Deep
Learning libraries like Tensorflow and PyTorch respectively. These
libraries along with Scikit-learn, SciPy, spaCy etc. already dominate
modern AI. Open source is not only effective in software, but also for
curating large datasets. We have platforms like Kaggle and CodaLab where
individuals, organisations or companies publish their datasets to obtain
a community-sourced solution. This also enables and encourages discourse
on any existing bias. Openness will lead to awareness. It would only
educate those unaware of algorithmic bias and help them understand its
implications.</p><figure><span class="gatsby-resp-image-wrapper" style="position:relative;display:block;margin-left:auto;margin-right:auto;max-width:590px">
      <span class="gatsby-resp-image-background-image" style="padding-bottom:55.4054054054054%;position:relative;bottom:0;left:0;background-image:url(&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACn0lEQVQozy2STW/iVhSG/eu67F+o2t/QfdVK7aJSRxpV6rKarjqjJlU7k1GUzOSDBAgYYgjEYBw+g8G+9vW1sWMGSMJT4c7i0Tlnc6RznleTUqKUYu4KGuYdtZZFpdGhbPxPvW3T7PSJ4oRQxTkiCHFFgOcHzD2BM/ewhxOE76PJMCSOIkIVoTe7HJxWeHdW5ejymuOSwdvTal6LhkmtbXOut9BvLMqNDgW9xceywYfiNadXDYIgQIuUwrCH/Lh3wHf77+lOpnxaLlmtVmw2mxy1WBKoBX4YMxeSe9fHE5IgVIhAMvMEo/sZQSDREqV4U6rx1Q8v+PbVPkVrTJqkPD095WyBTSbZ5h2U+yNeVmuUhmM6vRFlw6Rp3uEKnyiK0IQfoBttdGtIqXzJdfUDy9WG9epTvsDrHdN+/SX+7Z/5/E/rlu9LZd5bNsZNj6Jxiz2+J02T3IU2cebYgzH6jU1FL1G6eMfICRhMHIbziFFjj8Nfv2BQ/Z0w2eCriKI9wFMRj5s1tjWjVOghhCSOI7RASupti7NKgzeHRQ4vm6w//2+73ZKtH3GnJo/bZxwvRMUJgUqIFym+DDk/tjg7sDHNCWm6QAt3luOY/njKYUHPjTq+gucn1lvw9v7G/+lnwuGEZLUhTRJmtsUyy0izDMuaUr26w5l57CKo7e5udfoMRlNGjkvhooJ+coJUCx6fn/F+eYH3zdd45QoZENRLLP76DVm/pD/zKehNKk2Tk7KB2RugqUjxx95H3h5dMXYFlfML6vuvmQnJLFB0qwbdgyPGjkuyXOG1rvH/fYV3UyN+WDJzBa7nM5l+jo0QPh17mCd9l/ipK5jLiIeHjEAqPBWjlit8qZjOBd3hFLNrc3s3ZjhxyLIHFotFLmT3vv8A0CMTvWUUyMYAAAAASUVORK5CYII=&#x27;);background-size:cover;display:block"></span>
  <picture>
          <source srcSet="/static/e74173f0f4d6e02544a53b1aa9deffd2/cbe2e/datasets.webp 148w,/static/e74173f0f4d6e02544a53b1aa9deffd2/3084c/datasets.webp 295w,/static/e74173f0f4d6e02544a53b1aa9deffd2/5ca24/datasets.webp 590w,/static/e74173f0f4d6e02544a53b1aa9deffd2/dad35/datasets.webp 885w,/static/e74173f0f4d6e02544a53b1aa9deffd2/a9a89/datasets.webp 1024w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp"/>
          <source srcSet="/static/e74173f0f4d6e02544a53b1aa9deffd2/12f09/datasets.png 148w,/static/e74173f0f4d6e02544a53b1aa9deffd2/e4a3f/datasets.png 295w,/static/e74173f0f4d6e02544a53b1aa9deffd2/fcda8/datasets.png 590w,/static/e74173f0f4d6e02544a53b1aa9deffd2/efc66/datasets.png 885w,/static/e74173f0f4d6e02544a53b1aa9deffd2/2bef9/datasets.png 1024w" sizes="(max-width: 590px) 100vw, 590px" type="image/png"/>
          <img class="gatsby-resp-image-image" src="/static/e74173f0f4d6e02544a53b1aa9deffd2/fcda8/datasets.png" alt="White Defendants’ Risk Scores" title="White Defendants’ Risk Scores" loading="lazy" style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0"/>
        </picture>
    </span><figcaption style="text-align:center" aria-hidden="true">Some publicly available dataset sources.
<br/>
Image source: Great Learning <a href="#8">[8]</a></figcaption></figure><p>We have to recognize that issues regarding algorithmic fairness usually
cannot be foreseen or detected before launch <a href="#9">[9]</a>. It often includes
hindsight, and though wonderful, we need to identify any existing biases
before causing any catastrophic damage. There might not be one solution
that solves all problems. But the important point to keep in mind is
that we will have problems to solve. There have been communities
established for fairness, accountability and transparency (FAT ML
<a href="#10">[10]</a>). Tools are being developed to help us understand our data better
(Know Your Data <a href="#11">[11]</a>). Important metrics are being included to
evaluate fairness and mitigate any bias in trained models (AllenNLP
<a href="#12">[12]</a>, <a href="#13">[13]</a> and Fairlearn <a href="#14">[14]</a>). These steps are necessary to
ensure that we incorporate equity into our AI systems to prevent it from
amplifying inequalities.</p><p>While we all strive for equality, equity, fairness, justice, sometimes,
it is not enough. Sometimes we need to be biased to ensure a level
playing field for everyone. For example, the selective public schools of
New York. A New York Times article released an article titled, “Only 7
Black Students Got Into Stuyvesant, N.Y.’s Most Selective High School,
Out of 895 Spots,”. Eight of the nine specialized high schools require
applicants to undergo the Specialized High Schools Admissions Test
(SHSAT). The main problem is not that Black and Hispanic students
neglect to give the SHSAT, even though they comprise the majority of
students taking the test, they are disproportionately denied admission
into these specialized schools. Another fact of the matter is that
despite applying to these schools and taking entrance exams, children of
colour do not attend middle schools that funnel students into
specialized schools <a href="#15">[15]</a>. Here is another article that explains how
racism affects children of colour in public schools <a href="#16">[16]</a>.</p><p>Many questions arise from this unfortunate situation. Could we find
another way to fight this bias by working on an “anti-bias algorithm”?
Could an algorithm be trained on biased data, where it manages to
identify and disregard the bias? While these are hypotheticals, one
thing is certain is that if we have to integrate AI into society, we
have to keep algorithmic fairness in mind. But while algorithms and data
is one thing, addressing social and political themes are another problem
that we as a society have to solve.</p><h2 id="references" style="position:relative"><a href="#references" aria-label="references permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2><a name="1"></a>[1] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments,” Tech. Rep. 07-49, University of Massachusetts, Amherst, October 2007.<br/><br/><a name="2"></a>[2] “Why Racial Bias is Prevalent in Facial Recognition Technology.” <a target="_blank" href="http://jolt.law.harvard.edu/digest/why-racial-bias-is-prevalent-in-facial-recognition-technology">http://jolt.law.harvard.edu/digest/why-racial-bias-is-prevalent-in-facial-recognition-technology</a>. Accessed: 20th December 2021.<br/><br/><a name="3"></a>[3] “Machine Bias.” <a target="_blank" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>. Accessed: 20th December 2021.<br/><br/><a name="4"></a>[4] A. Lashbrook, “AI-Driven Dermatology Could Leave Dark-Skinned Patients Be- hind.” <a target="_blank" href="https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/">https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/</a>, Aug 2018. Accessed: 20th December 2021.<br/><br/><a name="5"></a>[5] “Facebook’s ad-serving algorithm discriminates by gender and race.” <a target="_blank" href="https://www.technologyreview.com/2019/04/05/1175/facebook-algorithm-discriminates-ai-bias/">https://www.technologyreview.com/2019/04/05/1175/facebook-algorithm-discriminates-ai-bias/</a>. Accessed: 20th December 2021.<br/><br/><a name="6"></a>[6] D. Leslie, A. Mazumder, A. Peppin, M. K. Wolters, and A. Hagerty, “Does “AI” stand for augmenting inequality in the era of covid-19 healthcare?,” BMJ, vol. 372, p. n304, March 2021.<br/><br/><a name="7"></a>[7] K. Holstein, J. Wortman Vaughan, H. Daumé, M. Dudik, and H. Wallach, “Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?,” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, p. 1–16, ACM, May 2019.<br/><br/><a name="8"></a>[8] “Great Learning.” <a target="_blank" href="https://www.mygreatlearning.com/blog/sources-for-analytics-and-machine-learning-datasets/">https://www.mygreatlearning.com/blog/sources-for-analytics-and-machine-learning-datasets/</a>. Accessed: 5th January 2022.<br/><br/><a name="9"></a>[9] A. Woodruff, “10 things you should know about algorithmic fairness,” Interactions, vol. 26, p. 47–51, Jun 2019.<br/><br/><a name="10"></a>[10] “Fairness, Accountability, and Transparency in Machine Learning.” <a target="_blank" href="https://www.fatml.org/">https://www.fatml.org/</a>. Accessed: 4th January 2022.<br/><br/><a name="11"></a>[11] T. P. t. Google, “Know Your Data.” <a target="_blank" href="https://knowyourdata.withgoogle.com">https://knowyourdata.withgoogle.com</a>. Accessed: 4th January 2022.<br/><br/><a name="12"></a>[12] “Fairness and Bias Mitigation · A Guide to Natural Language Processing With AllenNLP.” <a target="_blank" href="https://guide.allennlp.org/fairness/">https://guide.allennlp.org/fairness/</a>. Accessed: 4th January 2022.<br/><br/><a name="13"></a>[13] “Fairness Metrics Allen NLP Documentation.” <a target="_blank" href="https://docs.allennlp.org/main/api/fairness/fairness_metrics/">https://docs.allennlp.org/main/api/fairness/fairness_metrics/</a>. Accessed: 4th January 2022.<br/><br/><a name="14"></a>[14] “Fairlearn.” <a target="_blank" href="https://fairlearn.org/">https://fairlearn.org/</a>. Accessed: 4th January 2022.<br/><br/><a name="15"></a>[15] “Why New York City’s Selective Public High Schools Are Neglecting to Reflect the City’s Actual Diversity.” <a target="_blank" href="https://raceandschools.barnard.edu/selectivehighschools/rita-2/">https://raceandschools.barnard.edu/selectivehighschools/rita-2/</a>. Accessed: 4th January 2022.<br/><br/><a name="16"></a>[16] “How Racism Affects Children of Color in Public Schools.” <a target="_blank" href="https://www.thoughtco.com/how-racism-affects-public-school-minorities-4025361">https://www.thoughtco.com/how-racism-affects-public-school-minorities-4025361</a>. Accessed: 4th January 2022.<br/><br/><a name="17"></a>[17] V. Warmerdam, “koaning.io: Naive Bias<a name="2"></a>[tm] and Fairness Tooling.” <a target="_blank" href="https://koaning.io/posts/just-another-dangerous-situation/">https://koaning.io/posts/just-another-dangerous-situation/</a>, 2021.<br/><br/><a name="18"></a>[18] CrashCourse, “Algorithmic Bias and Fairness: Crash Course AI #18.” <a target="_blank" href="https://www.youtube.com/watch?v=gV0_raKR2UQ">https://www.youtube.com/watch?v=gV0_raKR2UQ</a>, 2019. Accessed: 20th December 2021.<br/><br/><a name="19"></a>[19] V. Eubanks, “The Digital Poorhouse: Embracing habit in an automated world,” Harper’s Magazine, vol. January 2018, Jan 2018. Accessed: 20th December 2021.<br/><br/><a name="20"></a>[20] J. Sherman, “AI and machine learning bias has dangerous implications.” <a target="_blank" href="https://opensource.com/article/18/1/how-open-source-can-fight-algorithmic-bias">https://opensource.com/article/18/1/how-open-source-can-fight-algorithmic-bias</a>. Accessed: 20th December 2021.<br/><br/><a name="21"></a>[21] “Bias in machine learning: How to measure fairness in algorithms?” <a target="_blank" href="https://www.trilateralresearch.com/bias-in-machine-learning-how-to-measure-fairness-in-algorithms/">https://www.trilateralresearch.com/bias-in-machine-learning-how-to-measure-fairness-in-algorithms/</a>. Accessed: 4th January 2022.<div style="text-align:center;margin-top:2rem">Photo by <a href="https://unsplash.com/@markusspiske?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Markus Spiske</a> on <a href="https://unsplash.com/s/photos/algorithm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></div></div></section><footer class="Article__ArticleFooter-jwdbi9-1 lmhsJW"><div class="Bio__BioWrapper-d7fgaq-0 fgnlCs"><figure class="author-image"><div alt="Kevlyn Kadamala" style="background-image:url(&quot;/static/6bdc39ec665b4dc2ed4a01c3514a0afc/497c6/profile.png&quot;)" class="img"></div></figure><section><h4>About the author</h4><p class="Commons__Text-sc-1aaxjtz-1 Bio__BioText-d7fgaq-1 eiSJyb liguIv">
  Hello, I am Kevlyn Kadamala. 
  I'm a postgraduate student currently pursuing my MSc in Artificial Intelligence at NUI Galway.
  My interest majorly lies in Artificial Intelligence, Deep Learning and Reinforcement Learning.
  I am aspiring to become an AI Engineer and a Researcher.
  </p></section></div></footer></article></main><aside class="Wrapper-xmiwfw-0 hncJWA"><div style="background-color:#ffffffbf;padding:15px;border-radius:5px" shortname="kad99kev" config="[object Object]" id="disqus_thread"></div></aside><aside class="PrevNextPost__PreviewContainer-sc-1r1qyep-0 dMxoIk"><article class="PrevNextPost__Preview-sc-1r1qyep-1 dOZAZy"><a aria-label="View Deep Learning on the Apple Silicon article" href="/training-on-the-m1"><div style="background-image:url(&quot;/static/7c6c6d003d22d0602ceea7c595dab136/mac-mini.webp&quot;)" class="PrevNextPost__PreviewCover-sc-1r1qyep-2 kSsoVo"></div><div class="PrevNextPost__PreviewContent-sc-1r1qyep-3 hPZqEh"><header><h2>Deep Learning on the Apple Silicon</h2></header><section><p>At the end of last year, I decided to switch my Mac 2013 for the latest Mac Mini with the Apple Silicon. This meant that initially some of…</p></section><footer><span class="Commons__ReadingTimeContainer-sc-1aaxjtz-3 cbpkEw">3<!-- --> min read</span><span class="Commons__Bull-sc-1aaxjtz-2 eobgui"></span><div class="TagList__ListContainer-tgjq80-0 eSwQYU"><span to="/tags/apple" class="TagList__TagListItem-tgjq80-2 fAsDiK">apple</span>, <span to="/tags/ai" class="TagList__TagListItem-tgjq80-2 fAsDiK">ai</span></div></footer></div></a></article></aside></div><footer class="Footer__FooterWrapper-t92llg-0 iTZmMq"><nav><div class="footer-col"><h3 class="footer-title">Kevlyn Kadamala<!-- --> © <!-- -->2022</h3><p class="footer-item-text">Built with<!-- --> <a class="footer-link" href="https://www.gatsbyjs.org">Gatsby</a>.</p><p class="footer-item-text">Theme using<!-- --> <a class="footer-link" href="https://github.com/maxpou/gatsby-starter-morning-dew">gatsby-starter-morning-dew</a>.</p><p class="footer-item-text">Hosted with<!-- --> <span class="footer-heart" role="img" aria-label="Love">❤</span> <!-- -->by<!-- --> <a class="footer-link" href="https://github.com">GitHub</a>.</p></div><div class="footer-col"><h3 class="footer-title">Explore</h3><div class="footer-column-items"><span class="footer-item"><a class="footer-link" href="/">Blog</a></span><span class="footer-item"><a class="footer-link" href="/education">Education</a></span><span class="footer-item"><a class="footer-link" href="/experience">Experience</a></span><span class="footer-item"><a class="footer-link" href="/awards">Awards</a></span><span class="footer-item"><a class="footer-link" href="/about-me">About</a></span><span class="footer-item"><a class="footer-link" href="https://drive.google.com/file/d/10YcQ89ZYcr-7y1c7z7oHM5SoZwCjrgxh/view?usp=sharing" rel="external">Resume</a></span></div></div><div class="footer-col"><h3 class="footer-title">Follow me!</h3><div class="footer-column-items"><span class="footer-item"><a class="footer-link" href="https://github.com/kad99kev" rel="external">GitHub</a></span><span class="footer-item"><a class="footer-link" href="https://twitter.com/kad99kev" rel="external">Twitter</a></span><span class="footer-item"><a class="footer-link" href="https://www.linkedin.com/in/kevlyn-kadamala/" rel="external">LinkedIn</a></span></div></div></nav></footer></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5KZKWQ507L"></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-5KZKWQ507L', {"send_page_view":false});gtag('config', 'UA-173936491-1', {"send_page_view":false});
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/intro-to-algorithmic-bias";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-a55677043c370575cf59.js"],"app":["/app-2cc33f53fc0cb54ce6df.js"],"component---cache-caches-gatsby-plugin-offline-app-shell-js":["/component---cache-caches-gatsby-plugin-offline-app-shell-js-89a48f87b078441f3621.js"],"component---src-pages-404-js":["/component---src-pages-404-js-52df103e6e19546c655b.js"],"component---src-templates-blog-list-template-js":["/component---src-templates-blog-list-template-js-84b16eb1e0d212f61041.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-6753f08586774d75dbac.js"],"component---src-templates-page-js":["/component---src-templates-page-js-ca2832c84cbd5ebc073c.js"],"component---src-templates-tags-js":["/component---src-templates-tags-js-e664a9880a28bf430305.js"]};/*]]>*/</script><script src="/polyfill-a55677043c370575cf59.js" nomodule=""></script><script src="/component---src-templates-blog-post-js-6753f08586774d75dbac.js" async=""></script><script src="/6dae301c9709e8c5899acc3516b7dd8f43382179-66d819d73e5ed43a1a17.js" async=""></script><script src="/3a2f1ab89072400eb78a258db61265f1964eff8a-3380ef0e1c3d3a1e650c.js" async=""></script><script src="/app-2cc33f53fc0cb54ce6df.js" async=""></script><script src="/framework-ea89757ffca28d78b958.js" async=""></script><script src="/webpack-runtime-7190c2abaa7047efd8ad.js" async=""></script></body></html>