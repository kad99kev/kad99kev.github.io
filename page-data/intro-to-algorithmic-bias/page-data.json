{"componentChunkName":"component---src-templates-blog-post-js","path":"/intro-to-algorithmic-bias","result":{"data":{"post":{"excerpt":"Humans write algorithms and code that run on data collected from the\nreal world. Together, they may mimic or exaggerate any preexisting bias…","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"title\": \"An Introduction to Algorithmic Bias\",\n  \"date\": \"2022-01-06T00:00:00.000Z\",\n  \"language\": \"en\",\n  \"slug\": \"intro-to-algorithmic-bias\",\n  \"tags\": [\"ai\", \"ethics\"],\n  \"cover\": \"./FootballTeam.webp\",\n  \"generate-card\": false\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Humans write algorithms and code that run on data collected from the\\nreal world. Together, they may mimic or exaggerate any preexisting bias.\\nThis is what we call Algorithmic Bias. We could try to avoid collecting\\ndata that segregate people based on their gender, race or religion. But,\\nthey may still materialize as correlated features. For example,\\npurchasing records could correlate to gender and zip codes could\\ncorrelate to race. Bias could also arise due to the lack of relevant\\ndata. For example, in the famous dataset - \\u201CLabeled Faces in the Wild\\u201D\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#1\"\n  }, \"[1]\"), \" 83.5% of images are of white people \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#2\"\n  }, \"[2]\"), \". It also contains a\\nlimited number of children, no babies, very few adults above the age of\\n80, and a small proportion of women. It also states that several\\nethnicities have very little or no representation at all. The creators\\nof the LFW dataset mention that the dataset is not meant for commercial\\napplications. But are companies active in trying to identify any\\npre-existing biases in their datasets? Are they aware of how their\\nalgorithms interact with society? Algorithmic systems shape our lives,\\ninfluencing our opportunities in employment, education and finance. The\\ndata that we unknowingly provide them with often fuel these systems.\\nThis has been the source of the flame that has destroyed so many\\ninnocent lives.\"), mdx(\"p\", null, \"In 2016, ProPublica published its analysis of COMPAS (Correctional\\nOffender Management Profiling for Alternative Sanctions). This was an\\nalgorithm that not only assesses the risk of committing a crime in the\\nfuture but also around two dozen so-called \\u201Ccriminogenic needs\\u201D. It\\nranks the defendants either as low, medium or high risk in each category\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#3\"\n  }, \"[3]\"), \". The algorithm predicted that blacks are twice as likely as whites\\nto be labelled a higher risk and not re-offend. But it would make the\\nopposite mistake among whites. They were more likely labelled to be\\nlower risk but go on and commit more crimes.\"), mdx(\"figure\", null, mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"504px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABQklEQVQoz5VR23KDIBT035qHVNRwUS5GTZNp2o5NjIKCYqbf3qKpTfvWmZ3DsrAHWLwwFmEsAsJn8i9405D6MAGIui6YhYQHmDks5A+fpjdzEAsfJQBTgJmPqI+o8xMOEF22+og6A+Fgds7mgHAsCoDZehOz4sDyAxHFI0wmPY++HxVvdwC51lEiwHJyQDjN9yHhD/6mklpq2w7X57J6qxqph1PTlbUqL6obrq/n+vh+afQA6RbSbUC4u3aUpAGmKwBPddu0fdP1ytimc0RqO9VhrlIPyti6NfuXcg0T7xYd4SuAzlIrM7b9qIyVemh7x6WxylhHtJXaKjOa8eNYVutN7C25B5hhUcTpjqTFDCwKzDPMc5L+0okoIMtu1/7xzyFjNgOxLMme5jjvdXD/VQuiJF0QxgKyDIsCsexeX1a/zJ/uPdP0J9mXCwAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/8cb8d97d3a179be9277c8dd45ebedbd9/cbe2e/black-defendant.webp 148w\", \"/static/8cb8d97d3a179be9277c8dd45ebedbd9/3084c/black-defendant.webp 295w\", \"/static/8cb8d97d3a179be9277c8dd45ebedbd9/062aa/black-defendant.webp 504w\"],\n    \"sizes\": \"(max-width: 504px) 100vw, 504px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/8cb8d97d3a179be9277c8dd45ebedbd9/12f09/black-defendant.png 148w\", \"/static/8cb8d97d3a179be9277c8dd45ebedbd9/e4a3f/black-defendant.png 295w\", \"/static/8cb8d97d3a179be9277c8dd45ebedbd9/08115/black-defendant.png 504w\"],\n    \"sizes\": \"(max-width: 504px) 100vw, 504px\",\n    \"type\": \"image/png\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/8cb8d97d3a179be9277c8dd45ebedbd9/08115/black-defendant.png\",\n    \"alt\": \"Black Defendants’ Risk Scores\",\n    \"title\": \"Black Defendants’ Risk Scores\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \"), mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"style\": {\n      \"textAlign\": \"center\"\n    },\n    \"aria-hidden\": \"true\"\n  }, \"Black Defendants\\u2019 Risk Scores\")), mdx(\"figure\", null, mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"504px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABOUlEQVQoz42RbW+CMBSF+W+axdGifaFvCAjOOOcUxAJFZNtf36qk8cuWJU/S5t6ce09PvSBUQagglUGoAJH3yz/x7MEiH7OngKxfD9lmN5vTgEpIxF/cdlgxtDv5xF9U7eVYNVOA5iyybWqN/IYVQyqpSiERE4AKbfalni1CSAUgwsfc+vplyijmSR5QOfFRodtSm6rpsExolLI4f0bsPmUUONybrUnCpwAdz23VdNr0pTalbrXp347VarNTqxdIbAo+5j7mduLNlDdGR+UUoFPd1d21uQx1N2jT15fBDF9N/2Gun2VtDqd6X5y370W83vI4B0R4LndIBFVpGK1olN4hKsEixiImKmHLbCTORJzbmKj0Hv8NEOHwMUdiyZYZUYmrOAARD7ZvzFnkCEKF7M4Uy+Sx7ro/4m96Xc5ECu5nugAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/b4666fea93ba85a4aadcee23c4a443ca/cbe2e/white-defendant.webp 148w\", \"/static/b4666fea93ba85a4aadcee23c4a443ca/3084c/white-defendant.webp 295w\", \"/static/b4666fea93ba85a4aadcee23c4a443ca/062aa/white-defendant.webp 504w\"],\n    \"sizes\": \"(max-width: 504px) 100vw, 504px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/b4666fea93ba85a4aadcee23c4a443ca/12f09/white-defendant.png 148w\", \"/static/b4666fea93ba85a4aadcee23c4a443ca/e4a3f/white-defendant.png 295w\", \"/static/b4666fea93ba85a4aadcee23c4a443ca/08115/white-defendant.png 504w\"],\n    \"sizes\": \"(max-width: 504px) 100vw, 504px\",\n    \"type\": \"image/png\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/b4666fea93ba85a4aadcee23c4a443ca/08115/white-defendant.png\",\n    \"alt\": \"White Defendants’ Risk Scores\",\n    \"title\": \"White Defendants’ Risk Scores\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \"), mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"style\": {\n      \"textAlign\": \"center\"\n    },\n    \"aria-hidden\": \"true\"\n  }, \"White Defendants\\u2019 Risk Scores\")), mdx(\"p\", null, \"In 2018, researchers conducted a study with Convolution Neural Networks\\n(CNN). It detected potentially cancerous skin lesions better than the\\nstudy group that included 58 dermatologists. Yet, the data used to train\\nthe CNN came from fair-skinned populations in the US, Australia and\\nEurope. As a result, people of colour could either be misdiagnosed with\\nnonexistent skin cancers or they could be completely missed \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#4\"\n  }, \"[4]\"), \". In\\n2019, Facebook was sued for letting advertisers target their ads based\\non race, gender and religion. It included postings for preschool\\nteachers and secretaries which they showed to a higher fraction of\\nwomen. And, they showed postings for janitors and taxi drivers to a\\nhigher proportion of minorities. White users were shown more ads on home\\nsales, while minorities were shown ads for rentals \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#5\"\n  }, \"[5]\"), \". The way these\\nsystems are established, create a positive feedback loop. Every\\nincorrect prediction, every false sentencing and every targeted\\nadvertisement adds another data point to the already biased system.\"), mdx(\"figure\", null, mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"590px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50.67567567567568%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEDAgT/xAAWAQEBAQAAAAAAAAAAAAAAAAACAAH/2gAMAwEAAhADEAAAAe7aeqpIh//EABkQAQACAwAAAAAAAAAAAAAAAAECEAARIf/aAAgBAQABBQJ4RmZumv/EABgRAAIDAAAAAAAAAAAAAAAAAAAhAQIR/9oACAEDAQE/AbKEaf/EABgRAAIDAAAAAAAAAAAAAAAAAAARARIx/9oACAECAQE/AU9KQf/EABcQAAMBAAAAAAAAAAAAAAAAAAARMSD/2gAIAQEABj8CKyY//8QAGxAAAwACAwAAAAAAAAAAAAAAAAEREDFBYcH/2gAIAQEAAT8h3JehKKpW0ieWx4Ef/9oADAMBAAIAAwAAABAf3//EABYRAQEBAAAAAAAAAAAAAAAAAAARcf/aAAgBAwEBPxDUW//EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPxBByD//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMXFRYf/aAAgBAQABPxBLBJGgd3ACTksVb8gl5F+RJscTV7P/2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/cbe2e/bmj.webp 148w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/3084c/bmj.webp 295w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/5ca24/bmj.webp 590w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/dad35/bmj.webp 885w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/2baf0/bmj.webp 1180w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/b84eb/bmj.webp 2000w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/a80bd/bmj.jpg 148w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c91a/bmj.jpg 295w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c72d/bmj.jpg 590w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/a8a14/bmj.jpg 885w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/fbd2c/bmj.jpg 1180w\", \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/3acf0/bmj.jpg 2000w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"type\": \"image/jpeg\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/39fc2a5282dc2d0337a8dfaa7a03aad8/1c72d/bmj.jpg\",\n    \"alt\": \"White Defendants’ Risk Scores\",\n    \"title\": \"White Defendants’ Risk Scores\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \"), mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"style\": {\n      \"textAlign\": \"center\"\n    },\n    \"aria-hidden\": \"true\"\n  }, \"The cascading effect of AI.\\n\", mdx(\"br\", {\n    parentName: \"figcaption\"\n  }), \"\\nImage source: The British Medical Journal \", mdx(\"a\", {\n    parentName: \"figcaption\",\n    \"href\": \"#6\"\n  }, \"[6]\"))), mdx(\"p\", null, \"The first step that we as developers can take is understanding our\\nresponsibility. A study was conducted in 2019 where one on one\\ninterviews were conducted with around 35 ML practitioners. Most of the\\ninterviewees reported that their teams do not have any protocols in\\nplace to support the collection and curation of a balanced or\\nrepresentative dataset. Often these teams do not discover serious\\nfairness issues until they receive customer feedback \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#7\"\n  }, \"[7]\"), \". A biased\\nsociety often leads to a biased AI, and programmers are merely a part of\\nsociety. This same AI that we use so often, is developed by human\\nprogrammers. Hence, we must recognize our own biases to avoid\\nincorporating them into the AI systems that we develop. So, how can we\\ncombat algorithmic bias? There has been a lot of discussion around the\\npower of open source and its communities. We could find great potential\\nin its technologies and methodologies in the fight against algorithmic\\nbias. We have AI research labs from companies like Google (Google Brain)\\nand Facebook (Facebook AI Research) working on open-sourced Deep\\nLearning libraries like Tensorflow and PyTorch respectively. These\\nlibraries along with Scikit-learn, SciPy, spaCy etc. already dominate\\nmodern AI. Open source is not only effective in software, but also for\\ncurating large datasets. We have platforms like Kaggle and CodaLab where\\nindividuals, organisations or companies publish their datasets to obtain\\na community-sourced solution. This also enables and encourages discourse\\non any existing bias. Openness will lead to awareness. It would only\\neducate those unaware of algorithmic bias and help them understand its\\nimplications.\"), mdx(\"figure\", null, mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"590px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"55.4054054054054%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACn0lEQVQozy2STW/iVhSG/eu67F+o2t/QfdVK7aJSRxpV6rKarjqjJlU7k1GUzOSDBAgYYgjEYBw+g8G+9vW1sWMGSMJT4c7i0Tlnc6RznleTUqKUYu4KGuYdtZZFpdGhbPxPvW3T7PSJ4oRQxTkiCHFFgOcHzD2BM/ewhxOE76PJMCSOIkIVoTe7HJxWeHdW5ejymuOSwdvTal6LhkmtbXOut9BvLMqNDgW9xceywYfiNadXDYIgQIuUwrCH/Lh3wHf77+lOpnxaLlmtVmw2mxy1WBKoBX4YMxeSe9fHE5IgVIhAMvMEo/sZQSDREqV4U6rx1Q8v+PbVPkVrTJqkPD095WyBTSbZ5h2U+yNeVmuUhmM6vRFlw6Rp3uEKnyiK0IQfoBttdGtIqXzJdfUDy9WG9epTvsDrHdN+/SX+7Z/5/E/rlu9LZd5bNsZNj6Jxiz2+J02T3IU2cebYgzH6jU1FL1G6eMfICRhMHIbziFFjj8Nfv2BQ/Z0w2eCriKI9wFMRj5s1tjWjVOghhCSOI7RASupti7NKgzeHRQ4vm6w//2+73ZKtH3GnJo/bZxwvRMUJgUqIFym+DDk/tjg7sDHNCWm6QAt3luOY/njKYUHPjTq+gucn1lvw9v7G/+lnwuGEZLUhTRJmtsUyy0izDMuaUr26w5l57CKo7e5udfoMRlNGjkvhooJ+coJUCx6fn/F+eYH3zdd45QoZENRLLP76DVm/pD/zKehNKk2Tk7KB2RugqUjxx95H3h5dMXYFlfML6vuvmQnJLFB0qwbdgyPGjkuyXOG1rvH/fYV3UyN+WDJzBa7nM5l+jo0QPh17mCd9l/ipK5jLiIeHjEAqPBWjlit8qZjOBd3hFLNrc3s3ZjhxyLIHFotFLmT3vv8A0CMTvWUUyMYAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/e74173f0f4d6e02544a53b1aa9deffd2/cbe2e/datasets.webp 148w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/3084c/datasets.webp 295w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/5ca24/datasets.webp 590w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/dad35/datasets.webp 885w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/a9a89/datasets.webp 1024w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/e74173f0f4d6e02544a53b1aa9deffd2/12f09/datasets.png 148w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/e4a3f/datasets.png 295w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/fcda8/datasets.png 590w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/efc66/datasets.png 885w\", \"/static/e74173f0f4d6e02544a53b1aa9deffd2/2bef9/datasets.png 1024w\"],\n    \"sizes\": \"(max-width: 590px) 100vw, 590px\",\n    \"type\": \"image/png\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/e74173f0f4d6e02544a53b1aa9deffd2/fcda8/datasets.png\",\n    \"alt\": \"White Defendants’ Risk Scores\",\n    \"title\": \"White Defendants’ Risk Scores\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \"), mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"style\": {\n      \"textAlign\": \"center\"\n    },\n    \"aria-hidden\": \"true\"\n  }, \"Some publicly available dataset sources.\\n\", mdx(\"br\", {\n    parentName: \"figcaption\"\n  }), \"\\nImage source: Great Learning \", mdx(\"a\", {\n    parentName: \"figcaption\",\n    \"href\": \"#8\"\n  }, \"[8]\"))), mdx(\"p\", null, \"We have to recognize that issues regarding algorithmic fairness usually\\ncannot be foreseen or detected before launch \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#9\"\n  }, \"[9]\"), \". It often includes\\nhindsight, and though wonderful, we need to identify any existing biases\\nbefore causing any catastrophic damage. There might not be one solution\\nthat solves all problems. But the important point to keep in mind is\\nthat we will have problems to solve. There have been communities\\nestablished for fairness, accountability and transparency (FAT ML\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#10\"\n  }, \"[10]\"), \"). Tools are being developed to help us understand our data better\\n(Know Your Data \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#11\"\n  }, \"[11]\"), \"). Important metrics are being included to\\nevaluate fairness and mitigate any bias in trained models (AllenNLP\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#12\"\n  }, \"[12]\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#13\"\n  }, \"[13]\"), \" and Fairlearn \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#14\"\n  }, \"[14]\"), \"). These steps are necessary to\\nensure that we incorporate equity into our AI systems to prevent it from\\namplifying inequalities.\"), mdx(\"p\", null, \"While we all strive for equality, equity, fairness, justice, sometimes,\\nit is not enough. Sometimes we need to be biased to ensure a level\\nplaying field for everyone. For example, the selective public schools of\\nNew York. A New York Times article released an article titled, \\u201COnly 7\\nBlack Students Got Into Stuyvesant, N.Y.\\u2019s Most Selective High School,\\nOut of 895 Spots,\\u201D. Eight of the nine specialized high schools require\\napplicants to undergo the Specialized High Schools Admissions Test\\n(SHSAT). The main problem is not that Black and Hispanic students\\nneglect to give the SHSAT, even though they comprise the majority of\\nstudents taking the test, they are disproportionately denied admission\\ninto these specialized schools. Another fact of the matter is that\\ndespite applying to these schools and taking entrance exams, children of\\ncolour do not attend middle schools that funnel students into\\nspecialized schools \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#15\"\n  }, \"[15]\"), \". Here is another article that explains how\\nracism affects children of colour in public schools \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#16\"\n  }, \"[16]\"), \".\"), mdx(\"p\", null, \"Many questions arise from this unfortunate situation. Could we find\\nanother way to fight this bias by working on an \\u201Canti-bias algorithm\\u201D?\\nCould an algorithm be trained on biased data, where it manages to\\nidentify and disregard the bias? While these are hypotheticals, one\\nthing is certain is that if we have to integrate AI into society, we\\nhave to keep algorithmic fairness in mind. But while algorithms and data\\nis one thing, addressing social and political themes are another problem\\nthat we as a society have to solve.\"), mdx(\"h2\", {\n    \"id\": \"references\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#references\",\n    \"aria-label\": \"references permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"References\"), mdx(\"a\", {\n    name: \"1\"\n  }), \"[1] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \\u201CLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments,\\u201D Tech. Rep. 07-49, University of Massachusetts, Amherst, October 2007.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"2\"\n  }), \"[2] \\u201CWhy Racial Bias is Prevalent in Facial Recognition Technology.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"http://jolt.law.harvard.edu/digest/why-racial-bias-is-prevalent-in-facial-recognition-technology\"\n  }, \"http://jolt.law.harvard.edu/digest/why-racial-bias-is-prevalent-in-facial-recognition-technology\"), \". Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"3\"\n  }), \"[3] \\u201CMachine Bias.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"\n  }, \"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"), \". Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"4\"\n  }), \"[4] A. Lashbrook, \\u201CAI-Driven Dermatology Could Leave Dark-Skinned Patients Be- hind.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/\"\n  }, \"https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/\"), \", Aug 2018. Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"5\"\n  }), \"[5] \\u201CFacebook\\u2019s ad-serving algorithm discriminates by gender and race.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.technologyreview.com/2019/04/05/1175/facebook-algorithm-discriminates-ai-bias/\"\n  }, \"https://www.technologyreview.com/2019/04/05/1175/facebook-algorithm-discriminates-ai-bias/\"), \". Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"6\"\n  }), \"[6] D. Leslie, A. Mazumder, A. Peppin, M. K. Wolters, and A. Hagerty, \\u201CDoes \\u201CAI\\u201D stand for augmenting inequality in the era of covid-19 healthcare?,\\u201D BMJ, vol. 372, p. n304, March 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"7\"\n  }), \"[7] K. Holstein, J. Wortman Vaughan, H. Daum\\xE9, M. Dudik, and H. Wallach, \\u201CImproving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?,\\u201D in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, p. 1\\u201316, ACM, May 2019.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"8\"\n  }), \"[8] \\u201CGreat Learning.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.mygreatlearning.com/blog/sources-for-analytics-and-machine-learning-datasets/\"\n  }, \"https://www.mygreatlearning.com/blog/sources-for-analytics-and-machine-learning-datasets/\"), \". Accessed: 5th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"9\"\n  }), \"[9] A. Woodruff, \\u201C10 things you should know about algorithmic fairness,\\u201D Interactions, vol. 26, p. 47\\u201351, Jun 2019.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"10\"\n  }), \"[10] \\u201CFairness, Accountability, and Transparency in Machine Learning.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.fatml.org/\"\n  }, \"https://www.fatml.org/\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"11\"\n  }), \"[11] T. P. t. Google, \\u201CKnow Your Data.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://knowyourdata.withgoogle.com\"\n  }, \"https://knowyourdata.withgoogle.com\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"12\"\n  }), \"[12] \\u201CFairness and Bias Mitigation \\xB7 A Guide to Natural Language Processing With AllenNLP.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://guide.allennlp.org/fairness/\"\n  }, \"https://guide.allennlp.org/fairness/\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"13\"\n  }), \"[13] \\u201CFairness Metrics Allen NLP Documentation.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://docs.allennlp.org/main/api/fairness/fairness_metrics/\"\n  }, \"https://docs.allennlp.org/main/api/fairness/fairness_metrics/\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"14\"\n  }), \"[14] \\u201CFairlearn.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://fairlearn.org/\"\n  }, \"https://fairlearn.org/\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"15\"\n  }), \"[15] \\u201CWhy New York City\\u2019s Selective Public High Schools Are Neglecting to Reflect the City\\u2019s Actual Diversity.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://raceandschools.barnard.edu/selectivehighschools/rita-2/\"\n  }, \"https://raceandschools.barnard.edu/selectivehighschools/rita-2/\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"16\"\n  }), \"[16] \\u201CHow Racism Affects Children of Color in Public Schools.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.thoughtco.com/how-racism-affects-public-school-minorities-4025361\"\n  }, \"https://www.thoughtco.com/how-racism-affects-public-school-minorities-4025361\"), \". Accessed: 4th January 2022.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"17\"\n  }), \"[17] V. Warmerdam, \\u201Ckoaning.io: Naive Bias\", mdx(\"a\", {\n    name: \"2\"\n  }), \"[tm] and Fairness Tooling.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://koaning.io/posts/just-another-dangerous-situation/\"\n  }, \"https://koaning.io/posts/just-another-dangerous-situation/\"), \", 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"18\"\n  }), \"[18] CrashCourse, \\u201CAlgorithmic Bias and Fairness: Crash Course AI #18.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.youtube.com/watch?v=gV0_raKR2UQ\"\n  }, \"https://www.youtube.com/watch?v=gV0_raKR2UQ\"), \", 2019. Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"19\"\n  }), \"[19] V. Eubanks, \\u201CThe Digital Poorhouse: Embracing habit in an automated world,\\u201D Harper\\u2019s Magazine, vol. January 2018, Jan 2018. Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"20\"\n  }), \"[20] J. Sherman, \\u201CAI and machine learning bias has dangerous implications.\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://opensource.com/article/18/1/how-open-source-can-fight-algorithmic-bias\"\n  }, \"https://opensource.com/article/18/1/how-open-source-can-fight-algorithmic-bias\"), \". Accessed: 20th December 2021.\", mdx(\"br\", null), mdx(\"br\", null), mdx(\"a\", {\n    name: \"21\"\n  }), \"[21] \\u201CBias in machine learning: How to measure fairness in algorithms?\\u201D \", mdx(\"a\", {\n    target: \"_blank\",\n    href: \"https://www.trilateralresearch.com/bias-in-machine-learning-how-to-measure-fairness-in-algorithms/\"\n  }, \"https://www.trilateralresearch.com/bias-in-machine-learning-how-to-measure-fairness-in-algorithms/\"), \". Accessed: 4th January 2022.\");\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"An Introduction to Algorithmic Bias","date":"2022-01-06T00:00:00.000Z","slug":"intro-to-algorithmic-bias","language":"en","tags":["ai","ethics"],"cover":{"publicURL":"/static/aae88b3745426292c0421afd229d473d/FootballTeam.webp"},"imageShare":null,"translations":null}}},"pageContext":{"slug":"intro-to-algorithmic-bias","previous":{"fileAbsolutePath":"/Users/kad99kev/Desktop/kad99kev.github.io/content/posts/training-on-the-m1/index.md","frontmatter":{"title":"Deep Learning on the Apple Silicon","slug":"training-on-the-m1","tags":["apple","ai"],"language":"en","cover":{"publicURL":"/static/7c6c6d003d22d0602ceea7c595dab136/mac-mini.webp"},"unlisted":null},"timeToRead":3,"excerpt":"At the end of last year, I decided to switch my Mac 2013 for the latest Mac Mini with the Apple Silicon. This meant that initially some of…"},"next":null}},"staticQueryHashes":["1956263691","4156811642"]}